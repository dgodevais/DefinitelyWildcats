{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Facial Features For Gender Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, log_loss, confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "DATAFOLDER = \"/Users/snuffles753/Documents/NYU-GSAS/ds1004/term-project/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pyspark related imports\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33990, 90001)\n"
     ]
    }
   ],
   "source": [
    "# Load the sparse matrix containing the image feature data\n",
    "b = np.load(os.path.join(DATAFOLDER, 'sparse_matrix.npz'))\n",
    "data = b['data']\n",
    "m_format = b['format']\n",
    "shape = b['shape']\n",
    "row = b['row']\n",
    "col = b['col']\n",
    "sp_face_features = sp.csr_matrix( (data,(row,col)), shape=shape )\n",
    "print(sp_face_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_dataframe_from_image_matrix(image_matrix, vector_length):\n",
    "    \"\"\"\n",
    "    Process the sparse scipy matrix with image features and return a spark dataframe with sparse vectors\n",
    "    \"\"\"\n",
    "    spark_rows_formatted = []\n",
    "    for i, row in enumerate(image_matrix):\n",
    "        indexes = list(map(lambda x: (x, 1), row.nonzero()[1]))\n",
    "        spark_rows_formatted.append( (i, indexes) )\n",
    "    mapped_f = map(lambda x: (x[0], Vectors.sparse(vector_length, x[1][1:])), spark_rows_formatted)\n",
    "    df_gender_analysis = spark.createDataFrame(mapped_f, schema=[\"index\", \"features\"])\n",
    "    return df_gender_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|index|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(90001,[4627,5527...|\n",
      "|    1|(90001,[4114,5114...|\n",
      "|    2|(90001,[2989,4001...|\n",
      "|    3|(90001,[1996,3098...|\n",
      "|    4|(90001,[4314,5215...|\n",
      "|    5|(90001,[3095,4294...|\n",
      "|    6|(90001,[4020,4818...|\n",
      "|    7|(90001,[4307,5306...|\n",
      "|    8|(90001,[3079,4081...|\n",
      "|    9|(90001,[2691,3991...|\n",
      "|   10|(90001,[3681,4483...|\n",
      "|   11|(90001,[2198,3499...|\n",
      "|   12|(90001,[2994,4092...|\n",
      "|   13|(90001,[3198,4297...|\n",
      "|   14|(90001,[2797,3401...|\n",
      "|   15|(90001,[2801,3580...|\n",
      "|   16|(90001,[2787,3889...|\n",
      "|   17|(90001,[3000,3510...|\n",
      "|   18|(90001,[3312,4312...|\n",
      "|   19|(90001,[2692,3705...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_image_features = get_spark_dataframe_from_image_matrix(sp_face_features[0:100], sp_face_features.shape[1])\n",
    "df_image_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------+\n",
      "|row_index|                file|gender|\n",
      "+---------+--------------------+------+\n",
      "|        0|cropped_10000217_...|   1.0|\n",
      "|        1|cropped_10000548_...|   1.0|\n",
      "|        2|cropped_100012_19...|   1.0|\n",
      "|        3|cropped_10001965_...|   1.0|\n",
      "|        4|cropped_10002116_...|   0.0|\n",
      "|        5|cropped_10002702_...|   0.0|\n",
      "|        6|cropped_10003541_...|   1.0|\n",
      "|        7|cropped_100039_19...|   1.0|\n",
      "|        8|cropped_10004113_...|   1.0|\n",
      "|        9|cropped_10004122_...|   1.0|\n",
      "|       10|cropped_10004299_...|   1.0|\n",
      "|       11|cropped_1000456_1...|   1.0|\n",
      "|       12|cropped_10004882_...|   1.0|\n",
      "|       13|cropped_1000522_1...|   0.0|\n",
      "|       14|cropped_10005261_...|   1.0|\n",
      "|       15|cropped_10005947_...|   0.0|\n",
      "|       16|cropped_1000684_1...|   1.0|\n",
      "|       17|cropped_10006850_...|   1.0|\n",
      "|       18|cropped_10007577_...|   1.0|\n",
      "|       19|cropped_1000781_1...|   1.0|\n",
      "+---------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_map = spark.read.csv(\"../data/wiki_data.csv\", header=True, inferSchema=True)\n",
    "file_map = file_map.select(file_map._c0.alias('row_index'), file_map.file, file_map.gender)\n",
    "file_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_map = pd.read_csv(os.path.join(DATAFOLDER, 'wiki_data.csv'),\n",
    "#                       names=['index2', 'f_name2', 'gender'],\n",
    "#                        header=1)\n",
    "# wiki_map['f_name2'] = wiki_map['f_name2'].str.replace('cropped_', '')\n",
    "# wiki_map.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = file_map.merge(wiki_map, left_on='f_name', right_on='f_name2', how='left')\n",
    "mapped.drop(['index', 'index2'], axis=1, inplace=True)\n",
    "mapped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = mapped[['gender']].values\n",
    "print(gender.shape)\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = face_features[:,1:]\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, gender, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test.shape)\n",
    "print(np.any(np.isnan(labels_train)))\n",
    "labels_train2 = np.nan_to_num(labels_train)\n",
    "print(np.any(np.isnan(labels_train2)))\n",
    "print(np.any(np.isnan(labels_test)))\n",
    "labels_test2 = np.nan_to_num(labels_test)\n",
    "print(np.any(np.isnan(labels_test2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling with scikit\n",
    "model = LogisticRegression()\n",
    "model.fit(data_train, labels_train2)\n",
    "y_pred = model.predict_proba(data_test)[:, 1]\n",
    "accuracy = accuracy_score(labels_test2, (y_pred > 0.5).astype(int))\n",
    "logloss = log_loss(labels_test2, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(labels_test2, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "metrics = {'Accuracy': accuracy, 'ROC AUC': roc_auc, 'Log Loss': logloss}\n",
    "plt.plot(fpr, tpr, label='AUC = {0:.3f}'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multilayer Perceptron approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark related imports\n",
    "import time\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sqlCtx.createDataFrame(data,).show()\n",
    "# rdd = sc.parallelize(data)\n",
    "sparse_matrix = Matrices.sparse(sp_face_features.shape[1],\n",
    "                                sp_face_features.shape[0],\n",
    "                                sp_face_features.indptr,\n",
    "                                sp_face_features.indices,\n",
    "                                sp_face_features.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(face_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# print(rdd.take(1))\n",
    "# print(sparse_matrix.toDense())\n",
    "# df = spark.createDataFrame([(1.0, 2.0, Vectors.dense(1.0)), (0.0, 2.0, Vectors.sparse(1, [], []))], \n",
    "#                            [\"label\", \"weight\", \"features\"])\n",
    "\n",
    "# df.show()\n",
    "test = convert_to_sparse_spark_format(sp_face_features)[0:100]\n",
    "# test = np_face_features[0:100, 0:80000]\n",
    "arr = np.array([[2,3,4], [2,8,5], [2,3,6],[4,5,7]])\n",
    "print(arr.shape)\n",
    "print(arr)\n",
    "print(face_features.shape)\n",
    "# print(test)\n",
    "print(type(arr))\n",
    "print(type(test))\n",
    "print(test[0][1:])\n",
    "# df = np.concatenate(arr).reshape(1000,-1)\n",
    "start_time = time.time()\n",
    "# dff = map(lambda x: (1, Vectors.sparse(90000, x)), test)\n",
    "dff = map(lambda x: (1, Vectors.sparse(90000, x[1:])), test)\n",
    "# dff = map(lambda x: (int(x.getrow()), 2, test)\n",
    "mydf = spark.createDataFrame(dff,schema=[\"label\", \"features\"])\n",
    "print(\"Model took {} seconds\".format((time.time() - start_time)))\n",
    "mydf.show(5)\n",
    "\n",
    "# df = map(lambda x: Vectors.dense(x), face_features)\n",
    "# df2 = spark.createDataFrame(df,[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test = convert_to_sparse_spark_format(sp_face_features)[0:100]\n",
    "mapped_gender = map(lambda x: (random.randint(0, 1), Vectors.sparse(90000, x[1:])), test)\n",
    "df_gender_analysis = spark.createDataFrame(mapped_gender, schema=[\"label\", \"features\"])\n",
    "df_gender_analysis.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"features2\"], outputCol=\"features\")\n",
    "output = assembler.transform(df_gender_analysis)\n",
    "formatted = output.select(output.label, output.features)\n",
    "# formatted  = formatted.limit(100)\n",
    "formatted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(df_gender_analysis)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients[0]))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

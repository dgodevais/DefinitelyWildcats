{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------+\n",
      "|file                                |gender|\n",
      "+------------------------------------+------+\n",
      "|cropped_10000217_1981-05-05_2009.jpg|1.0   |\n",
      "|cropped_10000548_1925-04-04_1964.jpg|1.0   |\n",
      "|cropped_100012_1948-07-03_2008.jpg  |1.0   |\n",
      "|cropped_10001965_1930-05-23_1961.jpg|1.0   |\n",
      "|cropped_10002116_1971-05-31_2012.jpg|0.0   |\n",
      "+------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "# import pyspark\n",
    "# import random\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example2\").getOrCreate()\n",
    "# This is for testing the connection\n",
    "labels = spark.read.csv(\"../data/wiki_data.csv\", header=True, inferSchema=True)\n",
    "labels = labels.drop(labels._c0)\n",
    "labels.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9']\n",
      "[StructField(_c0,StringType,true), StructField(_c1,DoubleType,true), StructField(_c2,DoubleType,true), StructField(_c3,DoubleType,true), StructField(_c4,DoubleType,true), StructField(_c5,DoubleType,true), StructField(_c6,DoubleType,true), StructField(_c7,DoubleType,true), StructField(_c8,DoubleType,true), StructField(_c9,DoubleType,true)]\n"
     ]
    }
   ],
   "source": [
    "df_images = spark.read.option(\n",
    "    \"maxColumns\", 10000\n",
    ").csv(\"data_processing/grayscale_data2.csv\", inferSchema=True)\n",
    "names = df_images.schema.names\n",
    "types = df_images.schema.fields\n",
    "print(names[0:10])\n",
    "print(types[0:10])\n",
    "names = names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                 _c0|                file|  _c1|\n",
      "+--------------------+--------------------+-----+\n",
      "|cropped_729282_19...|cropped_729282_19...| 50.0|\n",
      "|cropped_16744391_...|cropped_16744391_...|104.0|\n",
      "|cropped_33822611_...|cropped_33822611_...|176.0|\n",
      "|cropped_1531980_1...|cropped_1531980_1...| 22.0|\n",
      "|cropped_6932212_1...|cropped_6932212_1...|131.0|\n",
      "|cropped_159603_19...|cropped_159603_19...|131.0|\n",
      "|cropped_6217898_1...|cropped_6217898_1...| 94.0|\n",
      "|cropped_6617452_1...|cropped_6617452_1...|175.0|\n",
      "|cropped_39206675_...|cropped_39206675_...|109.0|\n",
      "|cropped_977529_19...|cropped_977529_19...|169.0|\n",
      "|cropped_4978095_1...|cropped_4978095_1...|217.0|\n",
      "|cropped_24998469_...|cropped_24998469_...|121.0|\n",
      "|cropped_16919959_...|cropped_16919959_...| 28.0|\n",
      "|cropped_3951105_1...|cropped_3951105_1...|113.0|\n",
      "|cropped_40229958_...|cropped_40229958_...|  6.0|\n",
      "|cropped_18067536_...|cropped_18067536_...| 13.0|\n",
      "|cropped_10529091_...|cropped_10529091_...|174.0|\n",
      "|cropped_6153619_1...|cropped_6153619_1...| 26.0|\n",
      "|cropped_1775031_1...|cropped_1775031_1...| 37.0|\n",
      "|cropped_2007335_1...|cropped_2007335_1...| 94.0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labeled = df_images.join(labels, df_images._c0 == labels.file, \"left_outer\")\n",
    "df_labeled = df_labeled.na.drop()\n",
    "tmp = df_labeled.select(df_labeled._c0, df_labeled.file, df_labeled._c1)\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[50.0,41.0,41.0,3...|\n",
      "|  1.0|[104.0,130.0,154....|\n",
      "|  0.0|[176.0,198.0,157....|\n",
      "|  1.0|[22.0,36.0,28.0,2...|\n",
      "|  1.0|[131.0,129.0,133....|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=names,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=['_c1', '_c2'],\n",
    "#     outputCol=\"features\")\n",
    "output = assembler.transform(df_labeled)\n",
    "formatted = output.select(output.gender.alias('label'), output.features)\n",
    "# formatted  = formatted.limit(100)\n",
    "formatted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatted  = formatted.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (2500,[],[])\n",
      "Intercept: 1.0582915115648464\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(formatted)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.5705697849717789\n",
      "+---+---+\n",
      "|FPR|TPR|\n",
      "+---+---+\n",
      "|0.0|0.0|\n",
      "|1.0|1.0|\n",
      "|1.0|1.0|\n",
      "+---+---+\n",
      "\n",
      "areaUnderROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
